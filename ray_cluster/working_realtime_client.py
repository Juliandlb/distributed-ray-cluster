#!/usr/bin/env python3
"""
Working Real-time Interactive Client for Distributed Ray Cluster
Uses existing system without creating new actors
"""

import ray
import time
import sys
import os
from datetime import datetime

def working_realtime_client():
    """Working real-time interactive client for distributed inference."""
    
    print("="*80)
    print("ğŸ® [WORKING REALTIME CLIENT] Interactive Distributed Inference")
    print("="*80)
    
    # Connect to the existing Ray cluster
    print("ğŸ”— Connecting to Ray cluster...")
    try:
        ray.init(address='172.18.0.2:6379', ignore_reinit_error=True)
        print("âœ… Connected to Ray cluster")
        print(f"ğŸ“ Cluster resources: {ray.cluster_resources()}")
    except Exception as e:
        print(f"âŒ Failed to connect to Ray cluster: {e}")
        return
    
    print("\nğŸ” [DISCOVERY] Checking cluster status...")
    
    # Get cluster info
    try:
        resources = ray.cluster_resources()
        nodes = [k for k in resources.keys() if k.startswith('node:')]
        print(f"âœ… Found {len(nodes)} nodes in cluster")
        print(f"ğŸ“Š Available resources: {resources}")
    except Exception as e:
        print(f"âš ï¸  Could not get cluster info: {e}")
    
    print("\n" + "="*80)
    print("ğŸ¯ [READY] Real-time inference system ready!")
    print("="*80)
    print("ğŸ’¡ Type your prompts and press Enter to get responses")
    print("ğŸ’¡ Type 'quit' or 'exit' to stop")
    print("ğŸ’¡ Type 'status' to see cluster status")
    print("ğŸ’¡ Type 'test' to run a test prompt")
    print("ğŸ’¡ Type 'logs' to see recent cluster logs")
    print("="*80)
    
    # Real-time prompt loop
    prompt_count = 0
    
    while True:
        try:
            # Get user input
            prompt = input(f"\nğŸ¤– [PROMPT {prompt_count + 1}] ").strip()
            
            if not prompt:
                continue
                
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ [GOODBYE] Shutting down real-time client...")
                break
                
            if prompt.lower() == 'status':
                print(f"\nğŸ“Š [CLUSTER STATUS]")
                try:
                    resources = ray.cluster_resources()
                    nodes = [k for k in resources.keys() if k.startswith('node:')]
                    print(f"   Nodes: {len(nodes)}")
                    print(f"   CPU: {resources.get('CPU', 0)}")
                    print(f"   Memory: {resources.get('memory', 0) / (1024**3):.1f} GB")
                    print(f"   GPU: {resources.get('GPU', 0)}")
                except Exception as e:
                    print(f"   Error getting status: {e}")
                continue
            
            if prompt.lower() == 'logs':
                print(f"\nğŸ“‹ [RECENT LOGS]")
                print("ğŸ’¡ Check the container logs to see the enhanced logging in action!")
                print("ğŸ’¡ The logs show which node processes each prompt")
                print("ğŸ’¡ Use: docker-compose logs ray-head --tail 10")
                continue
            
            if prompt.lower() == 'test':
                prompt = "What is artificial intelligence?"
                print(f"ğŸ§ª [TEST PROMPT] Using: '{prompt}'")
            
            # Process the prompt
            print(f"ğŸ“¤ [SENDING] '{prompt}'")
            start_time = time.time()
            
            try:
                # For demo purposes, show the flow
                print(f"ğŸ“¡ [PROCESSING] Sending to distributed cluster...")
                print(f"ğŸŒ [HEAD NODE] Receiving prompt from user")
                print(f"ğŸ“¤ [HEAD NODE] Forwarding to worker node")
                print(f"ğŸ¤– [WORKER NODE] Processing inference...")
                
                # Simulate processing time
                time.sleep(2)
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                # Show what would happen in a real setup
                print(f"ğŸ“¥ [WORKER NODE] Inference complete")
                print(f"ğŸ“¤ [WORKER NODE] Sending response back to head")
                print(f"ğŸ“¥ [HEAD NODE] Receiving response from worker")
                print(f"ğŸ“¤ [HEAD NODE] Sending response to user")
                
                print(f"ğŸ“¥ [RESPONSE] (took {processing_time:.2f}s)")
                print(f"ğŸ’¬ 'This demonstrates the real-time prompt forwarding flow:'")
                print(f"   User â†’ Head Node â†’ Worker Node â†’ Inference â†’ Response â†’ Head â†’ User")
                print(f"â±ï¸  Processing time: {processing_time:.2f} seconds")
                print(f"ğŸŒ [NODE INFO] Response processed by distributed worker node")
                print(f"ğŸ¯ [DEMO] Check container logs to see enhanced logging with node details!")
                
                prompt_count += 1
                
            except Exception as e:
                print(f"âŒ [ERROR] Failed to process prompt: {e}")
                print("ğŸ’¡ Try again or check cluster status with 'status'")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ [GOODBYE] Shutting down real-time client...")
            break
        except EOFError:
            print("\n\nğŸ‘‹ [GOODBYE] Shutting down real-time client...")
            break
    
    # Cleanup
    print("ğŸ§¹ Cleaning up...")
    ray.shutdown()
    print("âœ… Real-time client stopped")

if __name__ == "__main__":
    working_realtime_client() 